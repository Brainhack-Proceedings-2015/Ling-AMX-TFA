{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nmodule that implement Topographic Factor Analysis (TFA) model described in \\nManning, Jeremy R., et al. \"Topographic factor analysis: a Bayesian model for inferring brain networks from neural data.\" \\nPloS one 9.5 (2014).\\n'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "module that implement Topographic Factor Analysis (TFA) model described in \n",
    "Manning, Jeremy R., et al. \"Topographic factor analysis: a Bayesian model for inferring brain networks from neural data.\" \n",
    "PloS one 9.5 (2014).\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from __future__ import division \n",
    "import numpy as np\n",
    "from scipy.stats import norm\n",
    "from copy import deepcopy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def hyperparameter(location_arr):\n",
    "    '''\n",
    "    set the fixed hyperparamters \\pi\n",
    "    based on Table 2 in the original paper\n",
    "    location_arr: numpy.nd array, size no_voxels X dim\n",
    "    ---------------------\n",
    "    Return:\n",
    "    a dict, keys are names of the hyperparameters\n",
    "    values are cossponding fixed value\n",
    "    '''\n",
    "    pi_keys = ['sigma_y', 'mu_w', 'k_w', 'c', 'k_mu', 'mu_lambda', 'k_lambda']\n",
    "    pi = {key: None for key in pi_keys}\n",
    "    pi['sigma_y'] = 0.1\n",
    "    pi['mu_w'] = 0\n",
    "    pi['k_w'] = np.log(0.5)\n",
    "    pi['c'] = np.mean(location_arr, axis=0)\n",
    "    sigma_mu = np.var(location_arr, axis=0)\n",
    "    pi['k_mu'] = np.log(1/(10*sigma_mu))\n",
    "    pi['mu_lambda'] = 1\n",
    "    pi['k_lambda'] = np.log(1/3)\n",
    "    return pi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def variationalparameter():\n",
    "    '''\n",
    "    a dict representing variational parameter alpha\n",
    "    mu_w_n_k, k_w_n_k: size N X K\n",
    "    k_mu_k, mu_mu_k: K X D\n",
    "    rest: K\n",
    "    ----------------------\n",
    "    Return:\n",
    "    a dict, keys are names of the variational parameters\n",
    "    values are none\n",
    "    '''\n",
    "    alpha_keys = ['mu_w_n_k', 'k_w_n_k', 'mu_mu_k', 'k_mu_k', 'mu_lambda_k', 'k_lambda_k']\n",
    "    alpha = {key: None for key in alpha_keys}\n",
    "    return alpha"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def sample_from_normal(mu, k):\n",
    "    '''\n",
    "    random sample a point in one dimensional normal distribution \n",
    "    with mean=mu, var=exp(-k)\n",
    "    '''\n",
    "    return np.random.normal(mu, np.sqrt(np.exp(-k)),1).item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def initial_alpha(pi, N, K):\n",
    "    '''\n",
    "    initialize variational parameters alpha based on section 1.4\n",
    "    note, this is NOT a implementation of hotspot initialization on section 1.4.1\n",
    "    pi: dict, hyperparameter\n",
    "    N: number of images\n",
    "    K: number of source\n",
    "    --------------\n",
    "    Return:\n",
    "    dict, with initialized alpha values in dict\n",
    "    '''\n",
    "    alpha = variationalparameter()\n",
    "    alpha['k_w_n_k'] = np.ones((N,K))*np.log(10)\n",
    "    alpha['k_lambda_k'] = np.ones(K)*1\n",
    "    D = len(pi['k_mu'])\n",
    "    alpha['k_mu_k'] = np.ones((K, D))* (pi['k_mu']+3*np.log(10))\n",
    "    value_mu_mu_k = np.empty((K, D))\n",
    "    for k in np.arange(K):\n",
    "        value_mu_mu_k[k,:] = [sample_from_normal(c_i, k_i) for (c_i, k_i) in zip(pi['c'], pi['k_mu'])]\n",
    "    alpha['mu_mu_k'] = value_mu_mu_k\n",
    "    alpha['mu_lambda_k'] = np.array([sample_from_normal(pi['mu_lambda'], pi['k_lambda']) for _ in np.arange(K)])\n",
    "    alpha['mu_w_n_k'] = np.array([sample_from_normal(pi['mu_w'], pi['k_w']) for _ in np.arange(N*K)]).reshape(N,K)\n",
    "    return alpha "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def initial_eta(alpha):\n",
    "    '''\n",
    "    init eta dict, this is similar as alpha\n",
    "    '''\n",
    "    eta_keys = alpha.keys()\n",
    "    eta_values_shape = iter([v.shape for v in alpha.values()])\n",
    "    eta = {}\n",
    "    for key in eta_keys:\n",
    "        eta[key] = np.ones(eta_values_shape.next())\n",
    "    return eta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def sub_sampling_mask(N, V, N_sub, V_sub):\n",
    "    '''\n",
    "    sub sample part of the image to compute posterior\n",
    "    this is described in section 1.6\n",
    "    ------------------\n",
    "    Return:\n",
    "    binary mask array with size (N, V)\n",
    "    '''\n",
    "    sub_mask = np.zeros((N, V))\n",
    "    assert N_sub <= N\n",
    "    assert V_sub <= V \n",
    "    sub_sample_image_index = np.random.choice(N, N_sub, replace=False)\n",
    "    start_point = np.random.randint(V-V_sub+1)\n",
    "    for n in sub_sample_image_index:\n",
    "        sub_mask[n,start_point:(start_point+V_sub)] = 1\n",
    "    return sub_mask, sub_sample_image_index, start_point\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def one_sample_from_Q():\n",
    "    '''\n",
    "    a dict represents samples from q, it will include 3 keys\n",
    "    mu_k_d: K X D\n",
    "    k_lambda: K\n",
    "    w_n_sub_k: N X K\n",
    "    '''\n",
    "    sample_keys = ['mu_k_d', 'k_lambda', 'w_n_sub_k']\n",
    "    sample = {key: None for key in sample_keys}\n",
    "    return sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def sample_from_Q(alpha_0, sub_sample_image_index):\n",
    "    '''\n",
    "    Sample M samples from variantional distribution \n",
    "    this is described in S1, 1.7\n",
    "    -------------------\n",
    "    Return:\n",
    "    a simple dict with three keys\n",
    "    '''\n",
    "    K = alpha_0['k_mu_k'].shape[0]\n",
    "    D = alpha_0['k_mu_k'].shape[1]\n",
    "    sample = one_sample_from_Q()\n",
    "    sample['mu_k_d'] = np.array([sample_from_normal(mu_k_d, k_mu_k_d) for (mu_k_d, k_mu_k_d) in zip(alpha_0['mu_mu_k'].ravel(), alpha_0['k_mu_k'].ravel())]).reshape((K,D))\n",
    "    sample['k_lambda'] = np.array([sample_from_normal(mu_lambda_k, k_lambda_k) for (mu_lambda_k, k_lambda_k) in zip(alpha_0['mu_lambda_k'], alpha_0['k_lambda_k'])])\n",
    "    value_w_n_sub_k = np.zeros((len(sub_sample_image_index), K))\n",
    "    i = 0\n",
    "    for n in sub_sample_image_index:\n",
    "        mu = alpha_0['mu_w_n_k'][n,:]\n",
    "        k = alpha_0['k_w_n_k'][n,:]\n",
    "        value_w_n_sub_k[i,:] = [sample_from_normal(mu_i,k_i) for (mu_i, k_i) in zip(mu,k)]\n",
    "        i = i+1\n",
    "    sample['w_n_sub_k'] = value_w_n_sub_k\n",
    "    return sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def RBF(r, mu, Lambda):\n",
    "    '''\n",
    "    RBF kernel function described in equation (1)\n",
    "    and used in equation (3)\n",
    "    ----------------------\n",
    "    Return:\n",
    "    scalar \n",
    "    '''\n",
    "    assert len(r)==len(mu)==3\n",
    "    return np.exp((-(np.dot(np.transpose(r-mu), r-mu))/(np.exp(Lambda))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def F_posterior(sample, location_arr, start_point, V_sub):\n",
    "    '''\n",
    "    F_posterior matrix \n",
    "    --alpha: dictionary, format can be referred from TFA\n",
    "    --voxel: 2d array, V_sub*D, represent all voxels' location\n",
    "    --return: 2d array, K*V_sub\n",
    "    '''\n",
    "    mu_mat = sample['mu_k_d']\n",
    "    lambda_array = sample['k_lambda']\n",
    "    K = mu_mat.shape[0]\n",
    "    voxel = location_arr[start_point:(start_point+V_sub),:]\n",
    "    F_mat = np.empty((K, V_sub))\n",
    "    for i in np.arange(K):\n",
    "        for j in np.arange(V_sub):\n",
    "            F_mat[i,j] = RBF(voxel[j],mu_mat[i],lambda_array[i])\n",
    "    return F_mat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def logP(sample,Y,location_arr,sub_sample_image_index,start_point,V_sub, pi):\n",
    "    '''\n",
    "    log of the joint probability of the data and hidden variables\n",
    "    --sample: dictionary\n",
    "    --Y: observed data\n",
    "    '''\n",
    "    #log p(image)\n",
    "    F_mat = F_posterior(sample,location_arr, start_point, V_sub)\n",
    "    w_mat = sample['w_n_sub_k']\n",
    "    mu_n_sub_v_sub_mat = w_mat.dot(F_mat)\n",
    "    p1 = 0.0\n",
    "    sigma_y = pi['sigma_y'] \n",
    "    for i in np.arange(sub_sample_image_index.shape[0]):\n",
    "        for j in np.arange(V_sub):\n",
    "            p1+= normal_pdf_sigma(Y[sub_sample_image_index[i],(start_point+j)],mu_n_sub_v_sub_mat[i,j],sigma_y)\n",
    "    p1*=location_arr.shape[0]/V_sub\n",
    "    \n",
    "    #log p(weight)\n",
    "    mu_w = pi['mu_w']\n",
    "    k_mu = pi['k_w'] \n",
    "    p2 = np.sum([normal_pdf(w,mu_w, k_mu) for w in w_mat.ravel()])\n",
    "    \n",
    "    # log p(centers)\n",
    "    c = pi['c'] \n",
    "    k_mu = pi['k_mu'] \n",
    "    mu_k = sample['mu_k_d']\n",
    "    p3 = 0.0\n",
    "    for i in np.arange(mu_k.shape[0]):\n",
    "        p3 += np.sum([normal_pdf(x, mu, k) for (x, mu, k) in zip(mu_k[i,:], c, k_mu)])\n",
    "    \n",
    "    #log p(widths)\n",
    "    mu_lambda = pi['mu_lambda']\n",
    "    k_lambda  = pi['k_lambda'] \n",
    "    lambda_k = sample['k_lambda']\n",
    "    p4 = np.sum([normal_pdf(l, mu_lambda, k_lambda) for l in lambda_k ])\n",
    "    \n",
    "    return (p1+p2+p3+p4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def logQ(sample, alpha_0, sub_sample_image_index):\n",
    "    '''\n",
    "    compute log Q as described in S1 section 1.2\n",
    "    '''\n",
    "    q1 = 0.0\n",
    "    for sub_index, n in enumerate(sub_sample_image_index):\n",
    "        w = sample['w_n_sub_k'][sub_index, :]\n",
    "        mu = alpha_0['mu_w_n_k'][n,:]\n",
    "        k = alpha_0['k_w_n_k'][n,:]\n",
    "        q1 += np.sum([normal_pdf(w_i, mu_i,k_i) for (w_i, mu_i, k_i) in zip(w, mu,k)])\n",
    "\n",
    "    q2 = np.sum([normal_pdf(mu_k_d_i, mu_mu_k_i, k_mu_k_i) for (mu_k_d_i, mu_mu_k_i, k_mu_k_i) in zip(sample['mu_k_d'].ravel(), alpha_0['mu_mu_k'].ravel(), alpha_0['k_mu_k'].ravel())])\n",
    "    \n",
    "    q3 = np.sum([normal_pdf(k_lambda_i, mu_lambda_k_i, k_lambda_k_i) for (k_lambda_i, mu_lambda_k_i, k_lambda_k_i) in zip(sample['k_lambda'], alpha_0['mu_lambda_k'], alpha_0['k_lambda_k'])])\n",
    "    \n",
    "    return q1+q2+q3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def normal_pdf(x, mu, k):\n",
    "    '''\n",
    "    one dimensional normal pdf  \n",
    "    with mean=mu, var=exp(-k)\n",
    "    '''\n",
    "    return np.log(norm.pdf(x, loc=mu, scale=np.sqrt(np.exp(-k))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def normal_pdf_sigma(x, mu, sigma2):\n",
    "    '''\n",
    "    write me!\n",
    "    '''\n",
    "    return np.log(norm.pdf(x,loc=mu, scale=np.sqrt(sigma2)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def z1(s,mu,k):\n",
    "    return (s-mu)*np.exp(k)\n",
    "\n",
    "def z2(s,mu,k):\n",
    "    return 0.5 - 0.5*(s-mu)**2*np.exp(k)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def gradient_log_q_w_1(sample, alpha_0, sub_sample_image_index):\n",
    "    '''\n",
    "    to write!\n",
    "    '''\n",
    "    w_n_k = sample['w_n_sub_k']\n",
    "    mu_n_k = alpha_0['mu_w_n_k']\n",
    "    k_w_n_k = alpha_0['k_w_n_k']\n",
    "    N_sub = len(sub_sample_image_index)\n",
    "    K = w_n_k.shape[1]\n",
    "    z1_array = np.zeros((N_sub, K))\n",
    "    z2_array = np.zeros((N_sub, K))\n",
    "    for sub_index, n in enumerate(sub_sample_image_index):\n",
    "        w = w_n_k[sub_index,:]\n",
    "        mu = mu_n_k[n,:]\n",
    "        k = k_w_n_k[n,:]\n",
    "        z1_array[sub_index,:] = [z1(w_i, mu_i, k_i) for (w_i, mu_i, k_i) in zip(w, mu, k)]\n",
    "    return z1_array\n",
    "\n",
    "def gradient_log_q_w_2(sample, alpha_0, sub_sample_image_index):\n",
    "    '''\n",
    "    to write!\n",
    "    '''\n",
    "    w_n_k = sample['w_n_sub_k']\n",
    "    mu_n_k = alpha_0['mu_w_n_k']\n",
    "    k_w_n_k = alpha_0['k_w_n_k']\n",
    "    N_sub = len(sub_sample_image_index)\n",
    "    K = w_n_k.shape[1]\n",
    "    z1_array = np.zeros((N_sub, K))\n",
    "    z2_array = np.zeros((N_sub, K))\n",
    "    for sub_index, n in enumerate(sub_sample_image_index):\n",
    "        w = w_n_k[sub_index,:]\n",
    "        mu = mu_n_k[n,:]\n",
    "        k = k_w_n_k[n,:]\n",
    "        z2_array[sub_index,:] = [z2(w_i, mu_i, k_i) for (w_i, mu_i, k_i) in zip(w, mu, k)]\n",
    "    return z2_array\n",
    "\n",
    "def gradient_log_q_mu_1(sample, alpha_0):\n",
    "    '''\n",
    "    to write!\n",
    "    '''\n",
    "    mu_k_d = sample['mu_k_d']\n",
    "    K = mu_k_d.shape[0]\n",
    "    D = mu_k_d.shape[1]\n",
    "    mu_mu_k_d = alpha_0['mu_mu_k'] \n",
    "    k_mu_k_d = alpha_0['k_mu_k']\n",
    "    z1_array = np.array([z1(mu_k_d_i, mu_mu_k_d_i, k_mu_k_d_i) for(mu_k_d_i, mu_mu_k_d_i, k_mu_k_d_i) in \n",
    "                         zip(mu_k_d.ravel(), k_mu_k_d.ravel(), k_mu_k_d.ravel())]).reshape(K,D)\n",
    "    \n",
    "    return z1_array\n",
    "\n",
    "def gradient_log_q_mu_2(sample, alpha_0):\n",
    "    '''\n",
    "    to write!\n",
    "    '''\n",
    "    mu_k_d = sample['mu_k_d']\n",
    "    K = mu_k_d.shape[0]\n",
    "    D = mu_k_d.shape[1]\n",
    "    mu_mu_k_d = alpha_0['mu_mu_k'] \n",
    "    k_mu_k_d = alpha_0['k_mu_k']\n",
    "    z2_array = np.array([z2(mu_k_d_i, mu_mu_k_d_i, k_mu_k_d_i) for(mu_k_d_i, mu_mu_k_d_i, k_mu_k_d_i) in \n",
    "                         zip(mu_k_d.ravel(), k_mu_k_d.ravel(), k_mu_k_d.ravel())]).reshape(K,D)\n",
    "    \n",
    "    return z2_array\n",
    "\n",
    "def gradient_log_q_lambda_1(sample, alpha_0):\n",
    "    '''\n",
    "    to write!\n",
    "    '''\n",
    "    lambda_k = sample['k_lambda']\n",
    "    mu_lambda_k = alpha_0['mu_lambda_k']\n",
    "    k_lambda_k = alpha_0['k_lambda_k']\n",
    "    z1_array = np.array([z1(l,mu,k) for (l,mu,k) in zip(lambda_k, mu_lambda_k, k_lambda_k)])\n",
    "    return z1_array\n",
    "\n",
    "def gradient_log_q_lambda_2(sample, alpha_0):\n",
    "    '''\n",
    "    to write!\n",
    "    '''\n",
    "    lambda_k = sample['k_lambda']\n",
    "    mu_lambda_k = alpha_0['mu_lambda_k']\n",
    "    k_lambda_k = alpha_0['k_lambda_k']\n",
    "    z2_array = np.array([z2(l,mu,k) for (l,mu,k) in zip(lambda_k, mu_lambda_k, k_lambda_k)])\n",
    "    return z2_array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def name_fun_mapping(key):\n",
    "    '''\n",
    "    map string variable name (i.e. keys in alpha_0) to specific function to compute gradient\n",
    "    --------------\n",
    "    Return: fun\n",
    "    '''\n",
    "    mapping = {'mu_w_n_k': gradient_log_q_w_1, 'k_w_n_k': gradient_log_q_w_2,\n",
    "           'mu_mu_k': gradient_log_q_mu_1, 'k_mu_k': gradient_log_q_mu_1,\n",
    "           'mu_lambda_k': gradient_log_q_lambda_1, 'k_lambda_k': gradient_log_q_lambda_2}\n",
    "    return mapping[key]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def g_h(key, sample, alpha_0, Y,location_arr,sub_sample_image_index,start_point,V_sub, pi):\n",
    "    '''\n",
    "    compute g term for each random sample, this is described in table 5\n",
    "    '''\n",
    "    h_m_fun = name_fun_mapping(key)\n",
    "    fun_args = (sample, alpha_0, sub_sample_image_index) if key in ['mu_w_n_k', 'k_w_n_k'] else (sample, alpha_0)\n",
    "    h_m = h_m_fun(*fun_args)\n",
    "    log_p = logP(sample, Y,location_arr,sub_sample_image_index,start_point,V_sub, pi)\n",
    "    log_q = logQ(sample, alpha_0, sub_sample_image_index)\n",
    "    return (h_m * (log_p - log_q), h_m)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def G_H(key, M_samples, alpha_0, Y,location_arr,sub_sample_image_index,start_point,V_sub, pi):\n",
    "    '''\n",
    "    return G, H list for all M samples\n",
    "    '''\n",
    "    G_H_list = []\n",
    "    for sample in M_samples:\n",
    "        G_H_list.append(g_h(key, sample, alpha_0, Y,location_arr,sub_sample_image_index,start_point,V_sub, pi))\n",
    "    G_list = list(zip(*G_H_list)[0])\n",
    "    H_list = list(zip(*G_H_list)[1])\n",
    "    return (G_list, H_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def Beta(G_list,H_list):\n",
    "    '''\n",
    "    Beta is control variate:\n",
    "    '''\n",
    "    raveled_G = np.array([g.ravel() for g in G_list])\n",
    "    raveled_H = np.array([h.ravel() for h in H_list])\n",
    "    assert raveled_G.shape == raveled_H.shape\n",
    "    U = raveled_G.shape[1]\n",
    "    up = 0.0\n",
    "    down = 0.0\n",
    "    for i in np.arange(U):\n",
    "        up += np.cov(raveled_G[:,i], raveled_H[:,i])[0,0]\n",
    "        down += np.cov(raveled_G[:,i])\n",
    "    return up/down\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def gradient_estimate(key,M_samples,alpha_0,Y,location_arr,sub_sample_image_index,start_point,V_sub, pi):\n",
    "    '''\n",
    "    gradient estimate after collrolling variate\n",
    "    '''\n",
    "    G_list, H_list = G_H(key, M_samples, alpha_0, Y,location_arr,sub_sample_image_index,start_point,V_sub, pi)\n",
    "    beta = Beta(G_list, H_list)\n",
    "    assert G_list[0].shape == H_list[0].shape\n",
    "    assert len(G_list) == len(H_list)\n",
    "    grad_estimate = np.zeros(G_list[0].shape)\n",
    "    for m in np.arange(len(M_samples)):\n",
    "        grad_estimate += G_list[m]-beta*H_list[m]\n",
    "    return grad_estimate/len(M_samples)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def  delta_scalar(x,maxStepSize):\n",
    "    '''\n",
    "    compute the final delta to gradient, as described in Table 3\n",
    "    '''\n",
    "    return max(min(x, maxStepSize), -maxStepSize)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def delta_on_grad_estimate(grad_estimate, rho, maxStepSize, sub_sample_image_index):\n",
    "    '''\n",
    "    compute delta update for any gradient estimate\n",
    "    '''\n",
    "    delta = np.vectorize(delta_scalar)\n",
    "    try:\n",
    "        return delta(rho*grad_estimate, maxStepSize)\n",
    "    except ValueError:\n",
    "        return delta(rho[sub_sample_image_index,:]*grad_estimate, maxStepSize)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def update_alpha(key, alpha, alpha_0, delta, sub_sample_image_index):\n",
    "    '''\n",
    "    update alpha after looping through one key in alpha_0\n",
    "    '''\n",
    "    try:\n",
    "        alpha[key] = alpha_0[key] + delta\n",
    "    except ValueError:\n",
    "        alpha[key][sub_sample_image_index,:] = alpha_0[key][sub_sample_image_index,:] + delta\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def update_eta(key, eta, grad_estimate, sub_sample_image_index):\n",
    "    '''\n",
    "    update eta after looping through one key in alpha_0\n",
    "    '''\n",
    "    try:\n",
    "        eta[key] = eta[key] + grad_estimate**2\n",
    "    except ValueError:\n",
    "        eta[key][sub_sample_image_index,:] = eta[key][sub_sample_image_index,:] + grad_estimate**2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 234,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def SGD(Y,location_arr, K, eps):\n",
    "    '''\n",
    "    main SGD implementation, following steps in table 5\n",
    "    this doesn't implement hotspot initlization\n",
    "    Y: obsereved data\n",
    "    location_err: observed voxel location\n",
    "    K: number of sources\n",
    "    eps: tolerance parm\n",
    "    ---------------------------\n",
    "    Return: \n",
    "    alpha dict\n",
    "    '''\n",
    "    t = 0\n",
    "    maxStepSize = 1\n",
    "    N_sub = 10\n",
    "    V_sub = 5000\n",
    "    M = 2\n",
    "    max_delta = 1e10\n",
    "    gamma = 0.1 \n",
    "    N = Y.shape[0]\n",
    "    V = Y.shape[1]\n",
    "    pi = hyperparameter(location_arr)\n",
    "    alpha = initial_alpha(pi, N, K)\n",
    "    eta = initial_eta(alpha)\n",
    "    while max_delta > eps:\n",
    "        t += 1\n",
    "        print 'Iteration {}...'.format(t)\n",
    "        max_delta = 0.0\n",
    "        alpha_0 = deepcopy(alpha)\n",
    "        _, sub_sample_image_index, start_point = sub_sampling_mask(N, V, N_sub, V_sub)\n",
    "        M_samples = [sample_from_Q(alpha_0, sub_sample_image_index) for _ in np.arange(M)]\n",
    "        for key in alpha_0.keys():\n",
    "            print 'at alpha {}'.format(key)\n",
    "            rho = gamma/eta[key]\n",
    "            grad_estimate = gradient_estimate(key,M_samples,alpha_0,Y,location_arr,sub_sample_image_index,start_point,V_sub, pi)\n",
    "            delta = delta_on_grad_estimate(grad_estimate, rho, maxStepSize, sub_sample_image_index)\n",
    "            update_alpha(key, alpha, alpha_0, delta, sub_sample_image_index)\n",
    "            update_eta(key, eta, grad_estimate, sub_sample_image_index)\n",
    "            if np.max(np.abs(delta)) > max_delta:\n",
    "                max_delta = np.max(np.abs(delta))\n",
    "        max_delta = 0.0\n",
    "    return (alpha, eta)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 233,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def final_SGD(Y,location_arr, K, eps, alpha, eta):\n",
    "    '''\n",
    "    To ensure that all of the per-image source weights converge to local optima, \n",
    "    we set Nsub~N and maxStepSize \\infty, \n",
    "    fix all of the global parameters, and re-run the inference procedure until all of the local parameters converge.\n",
    "    '''\n",
    "    t = 0\n",
    "    maxStepSize = np.infty\n",
    "    N_sub = Y.shape[0]\n",
    "    V_sub = 5000\n",
    "    M = 2\n",
    "    max_delta = 1e10\n",
    "    gamma = 0.1 \n",
    "    N = Y.shape[0]\n",
    "    V = Y.shape[1]\n",
    "    pi = hyperparameter(location_arr)\n",
    "    while max_delta > eps:\n",
    "        t += 1\n",
    "        print 'Final update iteration {}...'.format(t)\n",
    "        max_delta = 0.0\n",
    "        alpha_0 = deepcopy(alpha)\n",
    "        _, sub_sample_image_index, start_point = sub_sampling_mask(N, V, N_sub, V_sub)\n",
    "        M_samples = [sample_from_Q(alpha_0, sub_sample_image_index) for _ in np.arange(M)]\n",
    "        for key in ['mu_w_n_k', 'k_w_n_k']:\n",
    "            print 'Final update at alpha {}'.format(key)\n",
    "            rho = gamma/eta[key]\n",
    "            grad_estimate = gradient_estimate(key,M_samples,alpha_0,Y,location_arr,sub_sample_image_index,start_point,V_sub, pi)\n",
    "            delta = delta_on_grad_estimate(grad_estimate, rho, maxStepSize, sub_sample_image_index)\n",
    "            update_alpha(key, alpha, alpha_0, delta, sub_sample_image_index)\n",
    "            update_eta(key, eta, grad_estimate, sub_sample_image_index)\n",
    "            if np.max(np.abs(delta)) > max_delta:\n",
    "                max_delta = np.max(np.abs(delta))\n",
    "        max_delta = 0.0\n",
    "    return alpha\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 235,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1...\n",
      "at alpha k_mu_k\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "global name 'M' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-235-1212828fd152>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m     \u001b[0mY\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'../../data/Y.npy'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[0mlocation_arr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'../../data/location_arr.npy'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m     \u001b[0malpha\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0meta\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mSGD\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mY\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mlocation_arr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m10\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0.01\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m     \u001b[0malpha_final\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfinal_SGD\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mY\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mlocation_arr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m10\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0.01\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0malpha\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0meta\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-234-8235cf1f273c>\u001b[0m in \u001b[0;36mSGD\u001b[0;34m(Y, location_arr, K, eps)\u001b[0m\n\u001b[1;32m     33\u001b[0m             \u001b[0;32mprint\u001b[0m \u001b[0;34m'at alpha {}'\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     34\u001b[0m             \u001b[0mrho\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgamma\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0meta\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 35\u001b[0;31m             \u001b[0mgrad_estimate\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgradient_estimate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mM_samples\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0malpha_0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mY\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mlocation_arr\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0msub_sample_image_index\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mstart_point\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mV_sub\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpi\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     36\u001b[0m             \u001b[0mdelta\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdelta_on_grad_estimate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgrad_estimate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrho\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmaxStepSize\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msub_sample_image_index\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     37\u001b[0m             \u001b[0mupdate_alpha\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0malpha\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0malpha_0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdelta\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msub_sample_image_index\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-227-81ce9c830cfa>\u001b[0m in \u001b[0;36mgradient_estimate\u001b[0;34m(key, M_samples, alpha_0, Y, location_arr, sub_sample_image_index, start_point, V_sub, pi)\u001b[0m\n\u001b[1;32m      4\u001b[0m     '''\n\u001b[1;32m      5\u001b[0m     \u001b[0mG_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mH_list\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mG_H\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mM_samples\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0malpha_0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mY\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mlocation_arr\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0msub_sample_image_index\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mstart_point\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mV_sub\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpi\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m     \u001b[0mbeta\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mBeta\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mG_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mH_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mM\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m     \u001b[0;32massert\u001b[0m \u001b[0mG_list\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mH_list\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m     \u001b[0;32massert\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mG_list\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mH_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: global name 'M' is not defined"
     ]
    }
   ],
   "source": [
    "if __name__ == '__main__':\n",
    "    Y = np.load('../../data/Y.npy')\n",
    "    location_arr = np.load('../../data/location_arr.npy')\n",
    "    alpha, eta = SGD(Y,location_arr, 10, 0.01)\n",
    "    alpha_final = final_SGD(Y,location_arr, 10, 0.01, alpha, eta)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'k_lambda_k': array([ 2.,  2.,  2.,  2.,  0.,  0.,  2.,  2.,  0.,  2.]),\n",
       " 'k_mu_k': array([[ 0.80071548,  0.39585898,  0.35862907],\n",
       "        [ 0.80071548,  0.39585898,  0.35862907],\n",
       "        [ 0.80071548,  0.39585898,  2.35862907],\n",
       "        [ 0.80071548,  0.39585898,  2.35862907],\n",
       "        [ 0.80071548, -1.60414102,  0.35862907],\n",
       "        [ 0.80071548,  0.39585898,  2.35862907],\n",
       "        [-1.19928452, -1.60414102,  2.35862907],\n",
       "        [ 0.80071548,  0.39585898,  2.35862907],\n",
       "        [ 0.80071548,  0.39585898,  2.35862907],\n",
       "        [ 0.80071548,  0.39585898,  0.35862907]]),\n",
       " 'k_w_n_k': array([[ 2.30258509,  2.30258509,  2.30258509, ...,  2.30258509,\n",
       "          2.30258509,  2.30258509],\n",
       "        [ 2.30258509,  2.30258509,  2.30258509, ...,  2.30258509,\n",
       "          2.30258509,  2.30258509],\n",
       "        [ 2.30258509,  2.30258509,  2.30258509, ...,  2.30258509,\n",
       "          2.30258509,  2.30258509],\n",
       "        ..., \n",
       "        [ 2.30258509,  2.30258509,  2.30258509, ...,  2.30258509,\n",
       "          2.30258509,  2.30258509],\n",
       "        [ 2.30258509,  2.30258509,  2.30258509, ...,  2.30258509,\n",
       "          2.30258509,  2.30258509],\n",
       "        [ 2.30258509,  2.30258509,  2.30258509, ...,  2.30258509,\n",
       "          2.30258509,  2.30258509]]),\n",
       " 'mu_lambda_k': array([-0.46626419,  5.1477774 ,  1.2036431 , -1.52860193, -0.87220108,\n",
       "         0.98117556, -1.47178708,  0.50526829,  1.23289701,  2.21734685]),\n",
       " 'mu_mu_k': array([[ 47.3876544 ,  59.56907875,  -5.57488817],\n",
       "        [ 29.45679927,  44.69969574,  -1.81241366],\n",
       "        [ 22.99158378,  61.28068757,  14.09130187],\n",
       "        [ 40.21271654,  55.10176641,  20.37761048],\n",
       "        [ 64.71175116, -67.51893016,  -0.51158016],\n",
       "        [ 28.99749268,  66.26461356,   5.78927126],\n",
       "        [ -6.67694543, -22.99226442,   7.36843102],\n",
       "        [ 50.16142972,  41.91252801,   8.60737953],\n",
       "        [ 50.68771016,  13.28303096,   3.98415791],\n",
       "        [ 40.01151837,  54.53393779, -11.73516798]]),\n",
       " 'mu_w_n_k': array([[ 0.72655864,  0.08357447,  2.25339522, ..., -1.75520064,\n",
       "          0.26386558, -0.19342236],\n",
       "        [-1.96424686, -0.8064437 ,  3.79296084, ...,  2.04754598,\n",
       "         -1.10632914, -0.90208959],\n",
       "        [-1.07614636,  2.91623496,  2.90086996, ..., -0.4566305 ,\n",
       "          2.46869532,  1.11955496],\n",
       "        ..., \n",
       "        [ 2.46592279,  0.88016927,  0.71140721, ...,  0.09613614,\n",
       "         -1.13276949, -2.66731972],\n",
       "        [ 0.59115273,  1.11425485, -0.24038734, ..., -0.33734811,\n",
       "         -1.58587802, -3.27218796],\n",
       "        [-1.23139853, -0.85549679,  1.10780258, ...,  1.95825411,\n",
       "         -2.31472564, -1.68795481]])}"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "alpha"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "with open('../../data/alpha.pickle', 'wb') as handle:\n",
    "    pickle.dump(alpha, handle)\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
